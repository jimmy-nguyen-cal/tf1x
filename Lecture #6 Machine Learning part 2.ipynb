{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import autograph #import autograph from contrib package to use graph mode\n",
    "%load_ext tensorboard\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from scipy import stats \n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Hw#2 Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single input node: #with this shape a[3,5]\n",
    "a = np.mat([[2,4,5,6,7], [4,2,6,7,9], [9,8,3,3,1]]) # numpy matrix\n",
    "a = tf.constant(a, dtype = np.int64, name=\"input_matrix\")\n",
    "\n",
    "#Operations\n",
    "b = tf.reduce_prod(a, name = \"prod_b\")\n",
    "c = tf.reduce_mean(a, name =\"mean\")\n",
    "e = tf.add(b,c, name=\"add_d\")\n",
    "d = tf.reduce_sum(a, name =\"sum_a\")\n",
    "f = tf.add(e,d, name=\"final_add\")\n",
    "\n",
    "# run the session\n",
    "sess = tf.Session()\n",
    "sess.run(f)\n",
    "\n",
    "# To create the graph:\n",
    "sess.graph.as_graph_def()\n",
    "file_writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "\n",
    "# We clean up before we exit()\n",
    "file_writer.close()\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3916), started 2:03:32 ago. (Use '!kill 3916' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ad09755fcc131384\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ad09755fcc131384\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=\"./graphs\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Generic frame of modeling in Machine Learning**\n",
    "\n",
    "Goals: \n",
    "- Focus on supervised learning problems, where we train an inference model with an input dataset, along with the real or expected output for each example\n",
    "- \"inference model\": is a series of mathematical operations that we apply to our data\n",
    "\n",
    "**Training Loop:**\n",
    "\n",
    "- Initializes the model parameters for the first time\n",
    "- Reads the traing data along with the expected output data for each data example\n",
    "- Executes the inference model on the training data, so it calculates for each training input example\n",
    "- Computes the loss\n",
    "- Adjusts the model parameters\n",
    "- loop repeats through several cycles\n",
    "\n",
    "**After training, we apply an evaluation phase**\n",
    "- we execute the inference against a different set data to which we also have the expected output and evaluate the loss for it.\n",
    "- Given how this dataset contains examples unknown for the model, the evaluation tells us how well the model predicts beyond its training.\n",
    "-  A very common practice is to take the original dataset and randomly split it in 70% of the examples for training, and 30% for evaluation\n",
    "\n",
    "---\n",
    "**Generic Frame for the model code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "# initialize variables/model parameters\n",
    "\n",
    "#define the training loop operations \n",
    "\n",
    "def inference(X):\n",
    "    pass\n",
    "    # compute inference model over data X and return the result \n",
    "\n",
    "def loss(X,Y):\n",
    "    pass\n",
    "    #Compute loss over training data X and expected outputs Y\n",
    "\n",
    "def inputs():\n",
    "    pass\n",
    "    # read/generate input training data X and expected data Y \n",
    "    \n",
    "def train(total_loss):\n",
    "    pass\n",
    "    # train/adjust model parameters according to computed total loss\n",
    "\n",
    "def evaluate(sess, X,Y):\n",
    "    pass\n",
    "    #evaluate the resulting trained model\n",
    "    \n",
    "    \n",
    "# Launch the graph in a session, setup boilerplate\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    X,Y = inputs()\n",
    "    \n",
    "    total_loss = loss(X,Y)\n",
    "    train_op = train(total_loss)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord =coord)\n",
    "    \n",
    "    #actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss: \", sess.run([total_loss]))\n",
    "    evaluate(sess, X, y)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    " \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Saving Checkpoints in TF**\n",
    " - Queing and threading in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Queuing and Threading:\n",
    "dummy_input = tf.random_normal([8], mean =0.5, stddev=2)\n",
    "dummy_input = tf.Print(dummy_input, data=[dummy_input], message=\"new dummy inputs have been created: \", summarize = 6)\n",
    "\n",
    "q = tf.FIFOQueue(capacity=3, dtypes=tf.float32)\n",
    "enqueue_op = q.enqueue_many(dummy_input)\n",
    "\n",
    "#Now set up a queue runner to handle enqueue_op outside of the main thread asynchronously\n",
    "qr = tf.train.QueueRunner(q, [enqueue_op] * 10)\n",
    "tf.train.add_queue_runner(qr)\n",
    "\n",
    "data = q.dequeue()\n",
    "data = tf.Print(data, data=[q.size(), data], message= \"this is how many items are left in q: \")\n",
    "#create a fake graph that we can call upon\n",
    "fg =data +1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord= coord)\n",
    "    #Now dequeue a few times, and we should see the number of items in the queue decrease\n",
    "    sess.run(fg)\n",
    "    sess.run(fg)\n",
    "    sess.run(fg)\n",
    "    # we have a queue runner on another thread making sure the queue is fill asynchronously \n",
    "    sess.run(fg)\n",
    "    sess.run(fg)\n",
    "    sess.run(fg)\n",
    "    # this will print, but not necessarily after the 6th call of sess.run(fg)\n",
    "    print(\"We're here!\")\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Saving Training checkpoints**\n",
    "\n",
    "\n",
    "- As we already stated, training models implies updating their parameters, or variables in Tensorflow, through many training cycles\n",
    "- Variables are stored in memory, so if the computer would lose power after many hours of training, we would lose all that work, so â€¦\n",
    "- We use tf.train.Saver class to save the graph variables in proprietary binary files\n",
    "- We should periodically save the variables, create a checkpoint file, and eventually restore the training from the most recent checkpoint if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "#model definition code ...\n",
    "\n",
    "# Create a saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Launch the graph in a session, setup boilerplate\n",
    "with tf.Session() sess:\n",
    "    # model set up ...\n",
    "    \n",
    "    initial_step = 0\n",
    "    # verify if we don't have a checkpoint saved already\n",
    "    cpkt = tf.train.get_checkpoint_state(os.path.dirname(__file__))\n",
    "        if cpkt and ckpt.model_checkpoint_path:\n",
    "            # restores from checkpint \n",
    "            saver.restore(sess, cpkt.model_checkpoint_path)\n",
    "            initial_step = int(cpkt.model_checkpoint_path.rsplit('-',1)[1]\n",
    "\n",
    "    #actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, 'my-model', global_step = step)\n",
    "            \n",
    "    #evaluation...\n",
    "    \n",
    "    saver.save(sess, \"my-model\", global_step = training_steps)\n",
    "    sess.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**\n",
    "\n",
    "```python\n",
    "\n",
    "Y= XW.T + b\n",
    "```\n",
    "\n",
    "**Weights**\n",
    "- However, instead of transposing weights, we can simply define them as a single column vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables/model parameters\n",
    "\n",
    "W = tf.Variable(tf.zeros([2,1]), name = \"weights\")\n",
    "b = tf.Variable(0., name=\"bias\")\n",
    "\n",
    "def inference(X):\n",
    "    return tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the loss**\n",
    "\n",
    "- For a simple model we can use the **squared error**:\n",
    "    - It sums the squared difference of all the predicted values for each training example with their corresponding expected values.\n",
    "    - Algebraically it is the squared Euclidean distance between the **predicted output vector and the expected one**\n",
    "- Graphically in a 2d dataset the error is the length of the vertical line that you can trace from the expected data point to the predicted regression line.\n",
    "- It is also known as **L2 norm or L2 loss function.**\n",
    "- We use it squared to avoid computing the square root, since it makes no difference for trying to minimize the loss and saves a computing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X,Y):\n",
    "    Y_predicted = inference(X)\n",
    "    return tf.reduce_sum(tf.squared_difference(Y,Y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training operation** \n",
    "- We will use the gradient descent algorithm for optimizing the model parameters:\n",
    "- when you run it, you will see printed how the loss gets smaller on each training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(total_loss):\n",
    "    learning_rate = 0.0000001\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "- Now that we trained the model, its time to evaluate it\n",
    "- Also, the output values are in-between the boundaries of the original trained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, X, Y):\n",
    "    print(sess.run(inference([[80., 25.]]))) # ~303\n",
    "    print(sess.run(inference([[65., 25.]]))) # ~ 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launch Graph in Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    X,Y = inputs()\n",
    "    total_loss = loss(X,Y)\n",
    "    train_op = train(total_loss)\n",
    "\n",
    "    #actual training loop\n",
    "    training_steps = 10000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        if step % 1000 == 0:\n",
    "            print(\"Epoch: \", step, \"loss: \", sess.run([total_loss]))\n",
    "    \n",
    "    print(\"Final model W= \", sess.run(W) \"b=\" , sess.run(b))\n",
    "    evaluate(sess, X, Y)\n",
    "\n",
    "    sess.close()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression in TF 1.x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "basic_graph = tf.Graph()\n",
    "with basic_graph.as_default():\n",
    "    a = tf.placeholder(name=\"a\",\n",
    "                      dtype= tf.float32,\n",
    "                      shape =(None, 5))\n",
    "    W = tf.Variable(tf.random_normal((5,3)), name = \"W\") #initialize it with random\n",
    "    b = tf.Variable(tf.random_normal((3,)), name=\"b\") # initialize it with random\n",
    "    \n",
    "#Create a session to run the graph\n",
    "with tf.Session(graph=basic_graph) as sess:\n",
    "    #writer =tf.summary.FileWriter('./graphs/basic_addition_graph', sess.graph)\n",
    "    #tensorboard --logdir=\"./graphs/basic_addition_graph -port=8800\"\n",
    "    print(sess.run(y, feed_dict={a: np.random.uniform(size = (500,5))}))\n",
    "    print(b.eval)\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "- sigmoid function\n",
    "        \n",
    "        f(x) = 1/(1+e)**-x\n",
    "        \n",
    " \n",
    " - In order to feed the function with the multiple dimensions, or features from the examples of our training datasets, we need to combine them into a single value\n",
    " \n",
    "```python\n",
    "\n",
    "# same parameters and variable initializations as log reg\n",
    "W = tf.Variable(tf.zeros([5,1]), name =\"weights\")\n",
    "b = tf.Variable(0., name=\"bias\")\n",
    "\n",
    "# former inference is now sued for combinding inputs\n",
    "def combine_inputs(X):\n",
    "    return tf.matmul(X,W) + b\n",
    "\n",
    "#new inferred value is the sigmoid applied to the former\n",
    "def inference(X):\n",
    "    return tf.sigmoid(combine_inputs(X))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1354abb50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.01 \n",
    "training_epochs = 500\n",
    "\n",
    "#Defining the sigmoid function:\n",
    "def sigmoid(x):\n",
    "    return 1./ (1. + np.exp(-x))\n",
    "\n",
    "# Create our data points on the x and y axis:\n",
    "x1 = np.random.normal(5,3,100)\n",
    "x2 = np.random.normal(-5, 3, 100)\n",
    "xs = np.append(x1, x2)\n",
    "ys = np.asarray([0.] * len(x1) + [1.] * len(x2))\n",
    "plt.scatter(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our parameters and palceholders for X and Y to feed them with the data above:\n",
    "X = tf.placeholder(tf.float32, shape=(None,), name=\"x\")\n",
    "Y =X = tf.placeholder(tf.float32, shape=(None,), name=\"y\")\n",
    "w = tf.Variable([0.,0.], name=\"parameter\", trainable = True)\n",
    "y_model = tf.sigmoid(-(w[1] * X + w[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(sess.run(y_model, feed_dict={X:xs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**not finished**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1x",
   "language": "python",
   "name": "tf1x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
